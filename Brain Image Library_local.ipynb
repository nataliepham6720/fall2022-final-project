{"cells":[{"cell_type":"markdown","id":"e2f41e84-896c-4a4a-bc4f-2d1fca75b8c5","metadata":{"id":"e2f41e84-896c-4a4a-bc4f-2d1fca75b8c5"},"source":["# Brain Image Library\n","\n","Name: Natalie Pham\n","\n","Andrew ID: thanhngp\n","\n","* You can work in groups, it is highly encouraged. Basic divide-and-conquer.\n","* But each of you will have to submit your notebook and data on Canvas.\n","\n","You have been given access to a database where you can pull information from one table. The purpose of this exercise is to\n","\n","* clean the local copy of the table\n","* compute some basic information about the files in the table\n","* add these features to the table\n","* use the original raw data to create a second table\n","* use these 2 tables to make some pretty plots\n","\n","In theory, the file and dataset level features you will be computing in this exercise will be stored in a database and used for a dashboard."]},{"cell_type":"markdown","id":"b11a9189-6ea9-45b0-8277-88ca4e17ca0f","metadata":{"id":"b11a9189-6ea9-45b0-8277-88ca4e17ca0f"},"source":["## Exercise 1\n","a. The variable `file` belows points to a CSV file with the file informations. Load the table into the workspace using Pandas or Dask.\n","\n","**Hint**\n","If you use Pandas, set `low_memory` to `False`. For more info click [here](https://stackoverflow.com/questions/58551446/how-to-set-low-memory-to-false)."]},{"cell_type":"code","source":["!pip install tqdm\n","!pip install pathlib\n","!pip install warnings"],"metadata":{"id":"3eQ3of33jBxM"},"id":"3eQ3of33jBxM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"abb12e3d","metadata":{"id":"abb12e3d"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","import pandas as pd\n","import time\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"id":"517f0557-ec0c-4fb1-aa19-2eb45eeb919c","metadata":{"id":"517f0557-ec0c-4fb1-aa19-2eb45eeb919c","outputId":"62d7e684-4566-45bd-d5f6-f2d7f954ddc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["20.26436424255371\n"]}],"source":["# INSERT CODE HERE\n","file = '/bil/workshops/2022/data-science/final_project_dataset.csv'\n","def load_data( file ):\n","    t1 = time.time()\n","    df = pd.read_csv(file, low_memory=False, sep=',' )\n","    t2 = time.time()\n","    total = t2-t1\n","    print(total)\n","    return df\n","\n","df = load_data(file)"]},{"cell_type":"code","execution_count":null,"id":"04497e21","metadata":{"id":"04497e21","outputId":"f0d92738-4d39-49ac-820f-775b4c4adf02"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2825872, 13)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dataset_uuid</th>\n","      <th>collection_id</th>\n","      <th>dataset_id</th>\n","      <th>sample_id</th>\n","      <th>directory</th>\n","      <th>filepath</th>\n","      <th>filename</th>\n","      <th>file_extension</th>\n","      <th>file_size</th>\n","      <th>file_creation_date</th>\n","      <th>sha256</th>\n","      <th>md5</th>\n","      <th>xxh128</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190895-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2019-12-0503:26:25</td>\n","      <td>2f83f9da49a4fc59b80b9185207055832653ddf51117e3...</td>\n","      <td>aae6752c964e56146934fef4e2e21491</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190895.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2019-12-0503:27:38</td>\n","      <td>55b2e61954ae32118f7d923a710489b0355123142579b9...</td>\n","      <td>2704360b304105e180fb08ef06d1a52b</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>xxhash_dat_mouseID_19022522-190895</td>\n","      <td>NaN</td>\n","      <td>1071128.0</td>\n","      <td>2019-12-0408:54:50</td>\n","      <td>906545fa0dead8e6a5496ba803e9acd2a2d439b66db691...</td>\n","      <td>fa2aaf565457ed5d5701d22d59f54506</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>191178</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>xxhash_dat_mouseID_19032517-191178</td>\n","      <td>NaN</td>\n","      <td>1096722.0</td>\n","      <td>2020-06-2907:43:20</td>\n","      <td>c4e07196782566e7773f69b1eba310492c2bd5355a273f...</td>\n","      <td>18758836aa7a3697dfcbc10ffc76dba2</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190896</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190896.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2020-06-2204:21:19</td>\n","      <td>cbca27551e57bba24c13aff4f8344614b5ff95fb2ac7c0...</td>\n","      <td>dd486c5107c9ee689df7588eedf399f6</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   dataset_uuid     collection_id dataset_id sample_id  \\\n","0           NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","1           NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","2           NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","3           NaN  009c1e6fcc03ebac    UNKNOWN    191178   \n","4           NaN  009c1e6fcc03ebac    UNKNOWN    190896   \n","\n","                                           directory  \\\n","0  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","1  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","2  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","3  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","4  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","\n","                                            filepath  \\\n","0  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","1  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","2  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","3  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","4  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","\n","                             filename file_extension  file_size  \\\n","0                 190895-metainfo.txt           .txt      693.0   \n","1                         190895.json          .json      534.0   \n","2  xxhash_dat_mouseID_19022522-190895            NaN  1071128.0   \n","3  xxhash_dat_mouseID_19032517-191178            NaN  1096722.0   \n","4                         190896.json          .json      534.0   \n","\n","   file_creation_date                                             sha256  \\\n","0  2019-12-0503:26:25  2f83f9da49a4fc59b80b9185207055832653ddf51117e3...   \n","1  2019-12-0503:27:38  55b2e61954ae32118f7d923a710489b0355123142579b9...   \n","2  2019-12-0408:54:50  906545fa0dead8e6a5496ba803e9acd2a2d439b66db691...   \n","3  2020-06-2907:43:20  c4e07196782566e7773f69b1eba310492c2bd5355a273f...   \n","4  2020-06-2204:21:19  cbca27551e57bba24c13aff4f8344614b5ff95fb2ac7c0...   \n","\n","                                md5 xxh128  \n","0  aae6752c964e56146934fef4e2e21491   None  \n","1  2704360b304105e180fb08ef06d1a52b   None  \n","2  fa2aaf565457ed5d5701d22d59f54506   None  \n","3  18758836aa7a3697dfcbc10ffc76dba2   None  \n","4  dd486c5107c9ee689df7588eedf399f6   None  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["print(df.shape)\n","df.head()"]},{"cell_type":"markdown","id":"029df5d5-0ada-4a05-818b-6429062b68b3","metadata":{"id":"029df5d5-0ada-4a05-818b-6429062b68b3"},"source":["b. Print the column names."]},{"cell_type":"code","execution_count":null,"id":"e73100a0-e9e2-4ed9-b5a2-b25b749a762d","metadata":{"id":"e73100a0-e9e2-4ed9-b5a2-b25b749a762d","outputId":"bb53091d-7c84-4a0c-edc9-e6bbfef80fb9"},"outputs":[{"data":{"text/plain":["Index(['dataset_uuid', 'collection_id', 'dataset_id', 'sample_id', 'directory',\n","       'filepath', 'filename', 'file_extension', 'file_size',\n","       'file_creation_date', 'sha256', 'md5', 'xxh128'],\n","      dtype='object')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# INSERT CODE HERE\n","df.keys()"]},{"cell_type":"markdown","id":"fd0409de-6eaf-4239-876c-392056e32abe","metadata":{"id":"fd0409de-6eaf-4239-876c-392056e32abe"},"source":["c. This dataframe has one line per file in the file system. The data wrangler already included some file level statistics in the table. However before we can proceed adding more columns, we need to fix some things.\n","\n","For example, if you take a look at the file creation dates, these look like\n","\n","```\n","df['file_creation_date'][0]\n","'2019-12-0503:26:25'\n","```\n","\n","when in fact these should look like \n","\n","```\n","'2019-12-05 03:26:25'\n","```\n","\n","First change the values in the column `file_creation_date` so the dates are correct.\n","\n","Second, change the data type of this series to be a `datetime` object.\n","\n","**Hint**\n","* Add a whitespace."]},{"cell_type":"code","execution_count":null,"id":"9db12f8a-c290-420d-b405-46da77dc2a07","metadata":{"id":"9db12f8a-c290-420d-b405-46da77dc2a07","outputId":"ed33ff91-6260-466c-9bf9-d2be1c330a79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing file_creation_date...\n","Time taken:  20.99143695831299\n"]}],"source":["# INSERT CODE HERE\n","def time_formatting(datetime):\n","    # datetime(str): 2019-12-0503:26:25\n","    \n","    # Output:\n","    # new_datetime: fixed format for date/time with proper spacing (2019-12-05 03:26:25)\n","    \n","    datetime = str(datetime)\n","    new_datetime = datetime[0:10] + ' ' + datetime[10:]\n","    return new_datetime\n","\n","start = time.time()\n","\n","print('Computing file_creation_date...')\n","\n","temp_file = 'manifest.tsv'\n","\n","if Path(temp_file).exists()==True:\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","else:\n","    df['file_creation_date'] = df.apply(lambda d: time_formatting(d.file_creation_date), axis=1)\n","    df['file_creation_date'] = pd.to_datetime(df['file_creation_date'],errors='coerce')\n","\n","end = time.time()\n","print('Time taken: ', end-start)"]},{"cell_type":"code","execution_count":null,"id":"de3c184f","metadata":{"id":"de3c184f","outputId":"4488b831-7a66-4677-9c4c-1547d137be97"},"outputs":[{"name":"stdout","output_type":"stream","text":["datetime64[ns]\n"]}],"source":["print(df['file_creation_date'].dtypes)"]},{"cell_type":"markdown","id":"1f829d4a-1032-420a-8f33-a288be5d15b6","metadata":{"id":"1f829d4a-1032-420a-8f33-a288be5d15b6"},"source":["d. The column `file_size` has the file size in bytes. Add a column named `human_readable_file_size` that is dtype string. This string representation of the file size should be an approximation to the nearest unit with one decimal point. For example, `15M`, `1.7G` and `5.6T`."]},{"cell_type":"code","execution_count":null,"id":"336b48ef-579c-4b19-8c93-283fca6eaae4","metadata":{"id":"336b48ef-579c-4b19-8c93-283fca6eaae4","outputId":"395af103-24b7-4904-b2b0-4fe0a575ce1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing human_readable_file_size...\n","Time taken:  21.508914470672607\n"]}],"source":["# INSERT CODE HERE\n","# add a new column 'human_readable_file_size'\n","df['human_readable_file_size'] = ''\n","\n","def file_size_conversion(size):\n","    # Input: \n","    # size (int): file size in bytes\n","    \n","    # Output:\n","    # r_size (str): approximate human readable file size (KB, MB, GB, TB)\n","    \n","    if np.isnan(size) == True or size < 0:\n","        r_size = ''\n","    elif size < 10**2: # 0 - 99 bytes\n","        r_size = str(round(size,1)) + 'B'\n","    elif size >= 10**2 and size < 10**5: # 100 - 99,999 bytes\n","        r_size = str(round(size/(10**3), 1)) + 'K'\n","    elif size >= 10**5 and size < 10**8: # 100,000 - 99,999,999 bytes\n","        r_size = str(round(size/(10**6), 1)) + 'M'\n","    elif size >= 10**8 and size < 10**11: # 100,000,000 - 99,999,999,999 bytes\n","        r_size = str(round(size/(10**9), 1)) + 'G'\n","    else: # above 100,000,000,000 bytes\n","        r_size = str(round(size/(10**12), 1)) + 'T'\n","    \n","    return(r_size)\n","\n","start = time.time()\n","\n","print('Computing human_readable_file_size...')\n","\n","temp_file = 'manifest.tsv'\n","\n","if Path(temp_file).exists()==True:\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","else:\n","    df['human_readable_file_size'] = df.apply(lambda s: file_size_conversion(s.file_size), axis=1)\n","\n","end = time.time()\n","print('Time taken: ', end-start)"]},{"cell_type":"code","execution_count":null,"id":"03d7e0a9","metadata":{"id":"03d7e0a9","outputId":"06490f00-ed4c-4d2b-89f1-630c3b7d0e66"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dataset_uuid</th>\n","      <th>collection_id</th>\n","      <th>dataset_id</th>\n","      <th>sample_id</th>\n","      <th>directory</th>\n","      <th>filepath</th>\n","      <th>filename</th>\n","      <th>file_extension</th>\n","      <th>file_size</th>\n","      <th>file_creation_date</th>\n","      <th>sha256</th>\n","      <th>md5</th>\n","      <th>xxh128</th>\n","      <th>human_readable_file_size</th>\n","      <th>exists</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190895-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2019-12-05 03:26:25</td>\n","      <td>2f83f9da49a4fc59b80b9185207055832653ddf51117e3...</td>\n","      <td>aae6752c964e56146934fef4e2e21491</td>\n","      <td>None</td>\n","      <td>0.7K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190895.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2019-12-05 03:27:38</td>\n","      <td>55b2e61954ae32118f7d923a710489b0355123142579b9...</td>\n","      <td>2704360b304105e180fb08ef06d1a52b</td>\n","      <td>None</td>\n","      <td>0.5K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190895</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>xxhash_dat_mouseID_19022522-190895</td>\n","      <td>NaN</td>\n","      <td>1071128.0</td>\n","      <td>2019-12-04 08:54:50</td>\n","      <td>906545fa0dead8e6a5496ba803e9acd2a2d439b66db691...</td>\n","      <td>fa2aaf565457ed5d5701d22d59f54506</td>\n","      <td>None</td>\n","      <td>1.1M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>191178</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>xxhash_dat_mouseID_19032517-191178</td>\n","      <td>NaN</td>\n","      <td>1096722.0</td>\n","      <td>2020-06-29 07:43:20</td>\n","      <td>c4e07196782566e7773f69b1eba310492c2bd5355a273f...</td>\n","      <td>18758836aa7a3697dfcbc10ffc76dba2</td>\n","      <td>None</td>\n","      <td>1.1M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190896</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190896.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2020-06-22 04:21:19</td>\n","      <td>cbca27551e57bba24c13aff4f8344614b5ff95fb2ac7c0...</td>\n","      <td>dd486c5107c9ee689df7588eedf399f6</td>\n","      <td>None</td>\n","      <td>0.5K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190896</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190896-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2020-06-22 04:20:03</td>\n","      <td>10d223a849f1c248c7c69a581bad3dc060c842bb40612c...</td>\n","      <td>7c6f929ca4f45fc98d4689dcb625f75c</td>\n","      <td>None</td>\n","      <td>0.7K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190896</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>xxhash_dat_mouseID_19022521-190896</td>\n","      <td>NaN</td>\n","      <td>992409.0</td>\n","      <td>2020-06-22 04:30:48</td>\n","      <td>0b4a31eadf12417698c9581d44849588b28b231986c3c8...</td>\n","      <td>bc7badfacf028e7c898c04ddb0d19493</td>\n","      <td>None</td>\n","      <td>1.0M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190896</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>readme.txt</td>\n","      <td>.txt</td>\n","      <td>51.0</td>\n","      <td>2020-06-22 04:12:15</td>\n","      <td>2d74dc07c14c812e0f67039fbe1d2a0e4891aaab687e37...</td>\n","      <td>c5d0dfcd8110324b88357c1862504b8d</td>\n","      <td>None</td>\n","      <td>51.0B</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>191180</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...</td>\n","      <td>xxhash_dat_mouseID_19032516-191180</td>\n","      <td>NaN</td>\n","      <td>1107522.0</td>\n","      <td>2020-06-29 07:53:40</td>\n","      <td>388f1184d4f622e848d1c9409f7c269728b13a393a97cd...</td>\n","      <td>8b5bd3e47273082d596de03ffbb1513b</td>\n","      <td>None</td>\n","      <td>1.1M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190897</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190897.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2020-06-22 05:07:25</td>\n","      <td>30b735a014794e5fa018a07a39e3abb13c3e7537cc0e6f...</td>\n","      <td>f8970d1ecfc5b6b793579e80ba08acd7</td>\n","      <td>None</td>\n","      <td>0.5K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190897</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190897-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2020-06-22 05:09:03</td>\n","      <td>5a7eaf273dcc10d346104c9918625910c4f29014200327...</td>\n","      <td>e6a9fdec5218b18b53fa5db02519a89d</td>\n","      <td>None</td>\n","      <td>0.7K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190897</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>xxhash_dat_mouseID_19022525-190897</td>\n","      <td>NaN</td>\n","      <td>1051278.0</td>\n","      <td>2020-06-22 05:31:25</td>\n","      <td>c179906466b03c3e7e9bf2f4961f31957202704db43374...</td>\n","      <td>75bcda81eb44035d498f1d71d4129d67</td>\n","      <td>None</td>\n","      <td>1.1M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190897</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>readme.txt</td>\n","      <td>.txt</td>\n","      <td>37.0</td>\n","      <td>2020-06-22 04:59:11</td>\n","      <td>b645acab17ae01c9d856929d79ceaa02aa849408ed69b3...</td>\n","      <td>3be0cb0979be3795f2aef1c2f9bb8fce</td>\n","      <td>None</td>\n","      <td>37.0B</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>191229</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19030...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19030...</td>\n","      <td>xxhash_dat_mouseID_19030613-191229</td>\n","      <td>NaN</td>\n","      <td>497466.0</td>\n","      <td>2020-06-27 22:24:23</td>\n","      <td>bbab032b3c6c75291b32610235b10f82999c1488ba5f1a...</td>\n","      <td>0f1efb9829cef279337b808005cc308c</td>\n","      <td>None</td>\n","      <td>0.5M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190898</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190898.json</td>\n","      <td>.json</td>\n","      <td>534.0</td>\n","      <td>2019-12-05 03:51:01</td>\n","      <td>fc13ef09ab8d27a057f034738e87ed10c95728eccf3254...</td>\n","      <td>e9b45e1453fb7bff32e23334ea0bad50</td>\n","      <td>None</td>\n","      <td>0.5K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190898</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190898-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2019-12-05 04:18:11</td>\n","      <td>898edfa289f9022599885847c0562969ef1d9746072958...</td>\n","      <td>33595f447afc3e2c58f58e35e8903520</td>\n","      <td>None</td>\n","      <td>0.7K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190898</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>xxhash_dat_mouseID_19022526-190898</td>\n","      <td>NaN</td>\n","      <td>1095504.0</td>\n","      <td>2019-12-04 06:51:25</td>\n","      <td>439eb42a12bdc50cb83a6c7ea327ee5e6f7e6e44298e20...</td>\n","      <td>9ddcb08c82eb054302ff98bf431b9a30</td>\n","      <td>None</td>\n","      <td>1.1M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>192853</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_w1905...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_w1905...</td>\n","      <td>xxhash_dat_mouseID_w19051015-192853</td>\n","      <td>NaN</td>\n","      <td>972486.0</td>\n","      <td>2020-06-27 22:42:16</td>\n","      <td>0f222ad0a3e4c2d3811329d8a9a2ec399660597305347b...</td>\n","      <td>64b90cabd802688f7d266a5d8fc63c9f</td>\n","      <td>None</td>\n","      <td>1.0M</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190900</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>190900-metainfo.txt</td>\n","      <td>.txt</td>\n","      <td>693.0</td>\n","      <td>2020-06-22 05:46:28</td>\n","      <td>3c5eb76bb40cbb11f514e38170d7b31c492dda30243d48...</td>\n","      <td>01700f25adf01fd9f1a0cdb9c4c0e191</td>\n","      <td>None</td>\n","      <td>0.7K</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>NaN</td>\n","      <td>009c1e6fcc03ebac</td>\n","      <td>UNKNOWN</td>\n","      <td>190900</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...</td>\n","      <td>readme.txt</td>\n","      <td>.txt</td>\n","      <td>37.0</td>\n","      <td>2020-06-22 05:49:26</td>\n","      <td>b645acab17ae01c9d856929d79ceaa02aa849408ed69b3...</td>\n","      <td>3be0cb0979be3795f2aef1c2f9bb8fce</td>\n","      <td>None</td>\n","      <td>37.0B</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    dataset_uuid     collection_id dataset_id sample_id  \\\n","0            NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","1            NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","2            NaN  009c1e6fcc03ebac    UNKNOWN    190895   \n","3            NaN  009c1e6fcc03ebac    UNKNOWN    191178   \n","4            NaN  009c1e6fcc03ebac    UNKNOWN    190896   \n","5            NaN  009c1e6fcc03ebac    UNKNOWN    190896   \n","6            NaN  009c1e6fcc03ebac    UNKNOWN    190896   \n","7            NaN  009c1e6fcc03ebac    UNKNOWN    190896   \n","8            NaN  009c1e6fcc03ebac    UNKNOWN    191180   \n","9            NaN  009c1e6fcc03ebac    UNKNOWN    190897   \n","10           NaN  009c1e6fcc03ebac    UNKNOWN    190897   \n","11           NaN  009c1e6fcc03ebac    UNKNOWN    190897   \n","12           NaN  009c1e6fcc03ebac    UNKNOWN    190897   \n","13           NaN  009c1e6fcc03ebac    UNKNOWN    191229   \n","14           NaN  009c1e6fcc03ebac    UNKNOWN    190898   \n","15           NaN  009c1e6fcc03ebac    UNKNOWN    190898   \n","16           NaN  009c1e6fcc03ebac    UNKNOWN    190898   \n","17           NaN  009c1e6fcc03ebac    UNKNOWN    192853   \n","18           NaN  009c1e6fcc03ebac    UNKNOWN    190900   \n","19           NaN  009c1e6fcc03ebac    UNKNOWN    190900   \n","\n","                                            directory  \\\n","0   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","1   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","2   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","3   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","4   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","5   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","6   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","7   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","8   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","9   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","10  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","11  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","12  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","13  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19030...   \n","14  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","15  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","16  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","17  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_w1905...   \n","18  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","19  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","\n","                                             filepath  \\\n","0   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","1   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","2   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","3   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","4   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","5   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","6   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","7   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","8   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032...   \n","9   /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","10  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","11  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","12  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","13  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19030...   \n","14  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","15  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","16  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","17  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_w1905...   \n","18  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","19  /bil/data/00/9c/009c1e6fcc03ebac/mouseID_19022...   \n","\n","                               filename file_extension  file_size  \\\n","0                   190895-metainfo.txt           .txt      693.0   \n","1                           190895.json          .json      534.0   \n","2    xxhash_dat_mouseID_19022522-190895            NaN  1071128.0   \n","3    xxhash_dat_mouseID_19032517-191178            NaN  1096722.0   \n","4                           190896.json          .json      534.0   \n","5                   190896-metainfo.txt           .txt      693.0   \n","6    xxhash_dat_mouseID_19022521-190896            NaN   992409.0   \n","7                            readme.txt           .txt       51.0   \n","8    xxhash_dat_mouseID_19032516-191180            NaN  1107522.0   \n","9                           190897.json          .json      534.0   \n","10                  190897-metainfo.txt           .txt      693.0   \n","11   xxhash_dat_mouseID_19022525-190897            NaN  1051278.0   \n","12                           readme.txt           .txt       37.0   \n","13   xxhash_dat_mouseID_19030613-191229            NaN   497466.0   \n","14                          190898.json          .json      534.0   \n","15                  190898-metainfo.txt           .txt      693.0   \n","16   xxhash_dat_mouseID_19022526-190898            NaN  1095504.0   \n","17  xxhash_dat_mouseID_w19051015-192853            NaN   972486.0   \n","18                  190900-metainfo.txt           .txt      693.0   \n","19                           readme.txt           .txt       37.0   \n","\n","     file_creation_date                                             sha256  \\\n","0   2019-12-05 03:26:25  2f83f9da49a4fc59b80b9185207055832653ddf51117e3...   \n","1   2019-12-05 03:27:38  55b2e61954ae32118f7d923a710489b0355123142579b9...   \n","2   2019-12-04 08:54:50  906545fa0dead8e6a5496ba803e9acd2a2d439b66db691...   \n","3   2020-06-29 07:43:20  c4e07196782566e7773f69b1eba310492c2bd5355a273f...   \n","4   2020-06-22 04:21:19  cbca27551e57bba24c13aff4f8344614b5ff95fb2ac7c0...   \n","5   2020-06-22 04:20:03  10d223a849f1c248c7c69a581bad3dc060c842bb40612c...   \n","6   2020-06-22 04:30:48  0b4a31eadf12417698c9581d44849588b28b231986c3c8...   \n","7   2020-06-22 04:12:15  2d74dc07c14c812e0f67039fbe1d2a0e4891aaab687e37...   \n","8   2020-06-29 07:53:40  388f1184d4f622e848d1c9409f7c269728b13a393a97cd...   \n","9   2020-06-22 05:07:25  30b735a014794e5fa018a07a39e3abb13c3e7537cc0e6f...   \n","10  2020-06-22 05:09:03  5a7eaf273dcc10d346104c9918625910c4f29014200327...   \n","11  2020-06-22 05:31:25  c179906466b03c3e7e9bf2f4961f31957202704db43374...   \n","12  2020-06-22 04:59:11  b645acab17ae01c9d856929d79ceaa02aa849408ed69b3...   \n","13  2020-06-27 22:24:23  bbab032b3c6c75291b32610235b10f82999c1488ba5f1a...   \n","14  2019-12-05 03:51:01  fc13ef09ab8d27a057f034738e87ed10c95728eccf3254...   \n","15  2019-12-05 04:18:11  898edfa289f9022599885847c0562969ef1d9746072958...   \n","16  2019-12-04 06:51:25  439eb42a12bdc50cb83a6c7ea327ee5e6f7e6e44298e20...   \n","17  2020-06-27 22:42:16  0f222ad0a3e4c2d3811329d8a9a2ec399660597305347b...   \n","18  2020-06-22 05:46:28  3c5eb76bb40cbb11f514e38170d7b31c492dda30243d48...   \n","19  2020-06-22 05:49:26  b645acab17ae01c9d856929d79ceaa02aa849408ed69b3...   \n","\n","                                 md5 xxh128 human_readable_file_size  exists  \n","0   aae6752c964e56146934fef4e2e21491   None                     0.7K    True  \n","1   2704360b304105e180fb08ef06d1a52b   None                     0.5K    True  \n","2   fa2aaf565457ed5d5701d22d59f54506   None                     1.1M    True  \n","3   18758836aa7a3697dfcbc10ffc76dba2   None                     1.1M    True  \n","4   dd486c5107c9ee689df7588eedf399f6   None                     0.5K    True  \n","5   7c6f929ca4f45fc98d4689dcb625f75c   None                     0.7K    True  \n","6   bc7badfacf028e7c898c04ddb0d19493   None                     1.0M    True  \n","7   c5d0dfcd8110324b88357c1862504b8d   None                    51.0B    True  \n","8   8b5bd3e47273082d596de03ffbb1513b   None                     1.1M    True  \n","9   f8970d1ecfc5b6b793579e80ba08acd7   None                     0.5K    True  \n","10  e6a9fdec5218b18b53fa5db02519a89d   None                     0.7K    True  \n","11  75bcda81eb44035d498f1d71d4129d67   None                     1.1M    True  \n","12  3be0cb0979be3795f2aef1c2f9bb8fce   None                    37.0B    True  \n","13  0f1efb9829cef279337b808005cc308c   None                     0.5M    True  \n","14  e9b45e1453fb7bff32e23334ea0bad50   None                     0.5K    True  \n","15  33595f447afc3e2c58f58e35e8903520   None                     0.7K    True  \n","16  9ddcb08c82eb054302ff98bf431b9a30   None                     1.1M    True  \n","17  64b90cabd802688f7d266a5d8fc63c9f   None                     1.0M    True  \n","18  01700f25adf01fd9f1a0cdb9c4c0e191   None                     0.7K    True  \n","19  3be0cb0979be3795f2aef1c2f9bb8fce   None                    37.0B    True  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.head(20)"]},{"cell_type":"markdown","id":"fa05f825-94c2-42ec-ad8a-506ef55739a8","metadata":{"id":"fa05f825-94c2-42ec-ad8a-506ef55739a8"},"source":["e. The column `xxh128` is empty. To compute this hash and populate this column we will use the function below. However, due to time constraints, we will only compute these hash for files whose extensions are not `.tif`/`.tiff`.\n","\n","For each file in the table, whose extension is not `.tif`/`.tiff`, compute the hash and store it in the column `xxh128`. Files without this hash, should remain as `None`."]},{"cell_type":"code","execution_count":null,"id":"e5624df5-ff17-43f9-85ab-754ba88fe591","metadata":{"id":"e5624df5-ff17-43f9-85ab-754ba88fe591","outputId":"fdf79820-4526-4098-e82d-65378359ec63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing xxh128...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2825872/2825872 [2:24:06<00:00, 326.84it/s]  "]},{"name":"stdout","output_type":"stream","text":["Time taken:  8646.156700611115\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#INSERT CODE HERE\n","import subprocess\n","\n","def compute_xxh128sum( filename ):\n","    if Path(str(filename)).is_file():\n","        # print(filename)\n","        results = subprocess.check_output('/bil/packages/xxhash/0.8.0/xxh128sum ' + str(filename) + ' | cut -d\" \" -f1 | xargs', shell=True,stderr=subprocess.DEVNULL)\n","        return results.decode(\"utf-8\").strip\n","\n","temp_file = 'manifest.tsv'\n","\n","if Path(temp_file).exists()==True:\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","    \n","start = time.time()\n","\n","print('Computing xxh128...')\n","\n","df1 = df.loc[(df['file_extension'].str.contains('.tif')!=True)]\n","xxh128 = df1.loc[(df['xxh128'] == 'None')]\n","print(xxh128.shape)\n","# Originally the column 'xxh128' has 1109984 values are None and their file extension are not '.tif'\n","# I have able to compute ~ 9000 new values for 'xxh128' \n","\n","counter = 0\n","if df2.shape[0] > 0:\n","    for i in tqdm(xxh128.index): \n","        # compute hash and store to 'xxh128' if file extension is not '.tif' or '.tiff'\n","        df['xxh128'][i] = compute_xxh128sum(df['filepath'][i])\n","\n","        # checkpoint to save df to a temporary tsv file\n","        if counter % 1000 == 0 or i == len(xxh128.index)-1:\n","            #print('checkpoint at ', i)\n","            df['xxh128'].to_csv('temp1e.tsv', sep=\"\\t\",index=False)\n","            counter = counter + 1\n","\n","end = time.time()\n","print('Time taken: ', end-start)"]},{"cell_type":"code","execution_count":null,"id":"24b81aee","metadata":{"id":"24b81aee"},"outputs":[],"source":["print(df['xxh128'].value_counts())"]},{"cell_type":"markdown","id":"be537bf1-6da9-4699-b899-b3aa21f6606c","metadata":{"id":"be537bf1-6da9-4699-b899-b3aa21f6606c"},"source":["f. Add column `exists`. Use the value in `filepath` to find if the file exists on disk. If the file exists, then populate the column as `True`. `False` otherwise.\n","\n","**Hint** \n","* Use `Path` from `pathlib`.\n","* All files should exist, if some of these don't, then report it. Not your fault."]},{"cell_type":"code","execution_count":null,"id":"8cdeb39c-aeb8-466a-bb09-05554ef6de5d","metadata":{"id":"8cdeb39c-aeb8-466a-bb09-05554ef6de5d","outputId":"763dc6c9-b031-48ad-e6ba-a1a8ce24c1b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing file_exists...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2825872/2825872 [2:02:17<00:00, 385.14it/s]  "]},{"name":"stdout","output_type":"stream","text":["Time taken:  7358.21319270134\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#INSERT CODE HERE\n","from pathlib import Path\n","\n","def check_filepath(filepath):\n","    try:\n","        exist = Path(str(filepath)).exists()\n","        return(exist)\n","    except PermissionError:\n","        exist = False\n","        return(exist)\n","\n","start = time.time()\n","\n","temp_file = 'manifest.tsv'\n","\n","print('Computing file_exists...')\n","\n","if Path(temp_file).exists():\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","else:\n","    # add column 'exist'\n","    df['exists'] = None\n","\n","for i in tqdm(df.index):\n","    # if np.isnan(df['exists'][i]) == True:\n","    if df['exists'][i] == None:\n","        df['exists'][i] = check_filepath(df['filepath'][i])\n","    \n","    # checkpoint to save df to a temporary tsv file\n","    if i % 5000 == 0 or i == len(df.index)-1:\n","        # print('checkpoint at ', i)\n","        df.to_csv('temp1f.tsv', sep=\"\\t\",index=False)\n","\n","end = time.time()\n","print('Time taken: ', end-start)"]},{"cell_type":"code","execution_count":null,"id":"107683af","metadata":{"id":"107683af","outputId":"ee710714-9edc-4292-e737-a6e4f352c9cb"},"outputs":[{"data":{"text/plain":["True     2824340\n","False       1532\n","Name: exists, dtype: int64"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# output the number of files exist on disk vs. non-exist\n","df['exists'].value_counts()"]},{"cell_type":"markdown","id":"f54cf773-c5c6-43e0-8fff-07051f8d8b9d","metadata":{"id":"f54cf773-c5c6-43e0-8fff-07051f8d8b9d"},"source":["g. Add columns `download_link`, `download_ready`, `response_code`, `download_timestamp`. Add these four columns and set the default values to `None`."]},{"cell_type":"code","execution_count":null,"id":"0fe3e463-b53a-48fb-9a04-7588557c7d56","metadata":{"id":"0fe3e463-b53a-48fb-9a04-7588557c7d56"},"outputs":[],"source":["#INSERT CODE HERE\n","df['download_link'] = None\n","df['download_ready'] = None\n","df['response_code'] = None\n","df['download_timestamp'] = None"]},{"cell_type":"markdown","id":"22e87c7f-1b0e-4859-b600-b361a78aec82","metadata":{"id":"22e87c7f-1b0e-4859-b600-b361a78aec82"},"source":["h. We can populate the columns above at the same time to minimize the number of requests. In this case, each value in `filepath` can be turned into an download link. For example, the file `/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032506-191184/readme.txt` can be turned into a URL by replacing the prefix `/bil/data` with `https://download.brainimagelibrary.org`, leading to `https://download.brainimagelibrary.org/00/9c/009c1e6fcc03ebac/mouseID_19032506-191184/readme.txt`.\n","\n","Download the response header (not the file) to determine if the file is reachable. Add `True` to `download_ready` if the file is reachable. `False` otherwise.\n","\n","Save the response code from the request to column `response_code` as an integer, e.g. 202, 404, etc.\n","\n","Record the timestamp when you made this request to column `download_timestamp`. This column should be of dtype `datetime`.\n","\n","**Hints**\n","* Write a single method that can perform these tasks in a single call.\n","* You could also write a method that takes the full dataframe and returns and updated dataframe.\n","* You should be saving checkpoints to avoid recomputation.\n","* All links should be reachable. If some of these aren't that's a problem, but not your problem.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"557aa499-fe40-4334-be88-b7b3d05a8923","metadata":{"id":"557aa499-fe40-4334-be88-b7b3d05a8923","outputId":"7eddcea2-ad34-4ce1-a411-17b8751b7139"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing download_ready...\n"]},{"name":"stderr","output_type":"stream","text":[" 76%|███████▋  | 2156373/2825872 [10:31:48<29:54:06,  6.22it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 77%|███████▋  | 2167493/2825872 [11:03:03<40:09:13,  4.55it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 77%|███████▋  | 2179564/2825872 [11:31:18<24:14:35,  7.41it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 78%|███████▊  | 2192999/2825872 [12:03:08<24:31:42,  7.17it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 78%|███████▊  | 2204298/2825872 [12:34:54<25:04:58,  6.88it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 78%|███████▊  | 2215339/2825872 [13:06:39<31:44:10,  5.34it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 79%|███████▉  | 2226473/2825872 [13:40:11<23:09:39,  7.19it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 79%|███████▉  | 2236653/2825872 [14:10:47<26:04:02,  6.28it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 80%|███████▉  | 2249484/2825872 [14:50:50<26:15:55,  6.10it/s] "]}],"source":["#INSERT CODE HERE\n","import requests\n","import datetime\n","\n","def check_download(filepath):\n","    path = str(filepath)[9:]\n","    # print(path)\n","    url = 'https://download.brainimagelibrary.org' + path\n","    response = requests.head(url)\n","    request_time = str(datetime.datetime.now())\n","    code = response.status_code\n","    if code == 200 or code == 308:\n","        download_ready = True\n","    else:\n","        download_ready = False\n","    \n","    return(url, download_ready, code, request_time)\n","\n","start = time.time()\n","\n","temp_file = 'manifest.tsv'\n","\n","print('Computing download_ready...')\n","if Path(temp_file).exists():\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","\n","for i in tqdm(df.index):    \n","    # get download link and check if link is ready for download\n","    if str(df['response_code'][i]) == 'nan':\n","        url, download_ready, code, request_time = check_download(df['filepath'][i])\n","        df['download_link'][i] = url\n","        df['download_ready'][i] = download_ready\n","        df['response_code'][i] = code\n","        df['download_timestamp'][i] = request_time\n","    \n","    # checkpoint to save df to a temporary tsv file\n","    if i % 10000 == 0 or i == len(df.index)-1:\n","        # print('checkpoint at ', i)\n","        df[['download_link','download_ready','response_code','download_timestamp']].to_csv('temp1h.tsv', sep=\"\\t\",index=False)\n","\n","df['download_timestamp'] = pd.to_datetime(df['download_timestamp'],errors='coerce')\n","end = time.time()\n","print('Time taken: ', end-start)"]},{"cell_type":"code","execution_count":null,"id":"bdd0a5c7","metadata":{"id":"bdd0a5c7"},"outputs":[],"source":["df['response_code'].value_counts()"]},{"cell_type":"markdown","id":"9344e556-170d-4ea6-9bc1-26ec9d5235f5","metadata":{"id":"9344e556-170d-4ea6-9bc1-26ec9d5235f5"},"source":["i. Populate the column `dataset_uuid`. This one is tricky. This dataframe has one row per file. These files are grouped together in datasets. However the column `dataset_uuid` is empty. And now we need to populate it.\n","\n","A dataset can be identified as a combination of `collection_id`, `sample_id` and `directory`. Generate a unique [UUID](https://www.educba.com/python-uuid/) for each unique combination of `collection_id`, `sample_id` and `directory`. Use the UUID to populate the column `dataset_uuid`. Keep in mind there will be multiple rows in this table that will share the combination of `collection_id`, `sample_id` and `directory`, hence all of these rows should also have the `dataset_uuid` as it means these are all part of the same dataset."]},{"cell_type":"code","execution_count":null,"id":"7c78dc38","metadata":{"id":"7c78dc38"},"outputs":[],"source":["import uuid\n","\n","print('Compute uuid...')\n","\n","temp_file = 'manifest.tsv'\n","\n","if Path(temp_file).exists():\n","    df = pd.read_csv( temp_file, sep='\\t', low_memory=False )\n","else:\n","    df['dataset_uuid'] = df.groupby(['collection_id', 'sample_id', 'directory']).dataset_uuid.transform(lambda f: uuid.uuid4())"]},{"cell_type":"markdown","id":"946f4e6a-29fc-46e5-bcdb-ec7040e2bb22","metadata":{"id":"946f4e6a-29fc-46e5-bcdb-ec7040e2bb22"},"source":["k. If you haven't done it (and you should have saved some checkpoints), then save the df to disk as a pickle file and as a tsv file. Save it to `manifest.tsv` and `manifest.pkl`."]},{"cell_type":"code","execution_count":null,"id":"0dab94ff","metadata":{"id":"0dab94ff"},"outputs":[],"source":["print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"4c57505c-c58d-43a5-b762-c6ff361838a0","metadata":{"id":"4c57505c-c58d-43a5-b762-c6ff361838a0"},"outputs":[],"source":["#INSERT CODE HERE\n","# save to .tsv file\n","df.to_csv('manifest.tsv', sep=\"\\t\",index=False)\n","\n","# save to .pkl file\n","import pickle\n","with open('manifest.pkl', 'wb') as handle:\n","    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","id":"a033c3f7-f95a-4f54-8e79-c84027b7c90b","metadata":{"id":"a033c3f7-f95a-4f54-8e79-c84027b7c90b"},"source":["## Exercise 2\n","a. Create another dataframe with columns `['dataset_uuid', 'collection_id', 'dataset_id', 'sample_id', 'directory', 'file_extensions','timestamp']`.\n","\n","i. Set `file_extensions` to `None`.\n","\n","ii. Use the values in the first dataframe to populate the other columns.\n","\n","iii. Record the timestamp when you made this request to column `timestamp`. This column should be of dtype `datetime`.\n","\n","**Hints**\n","* The first dataframe had a row per each file in the dataset. Whereas the second dataframe should have one row per dataset."]},{"cell_type":"code","execution_count":null,"id":"a8160077-c7cf-4a14-9022-f5d90855d132","metadata":{"id":"a8160077-c7cf-4a14-9022-f5d90855d132"},"outputs":[],"source":["#INSERT CODE HERE\n","import datetime\n","\n","df2 = df[['dataset_uuid', 'collection_id', 'dataset_id', 'sample_id', 'directory']]\n","df2['file_extensions'] = None\n","df2['timestamp'] = datetime.datetime.now()\n","\n","df2.drop_duplicates(subset=['dataset_uuid'], inplace=True)\n","print(df2.shape)\n","\n","df2.head()"]},{"cell_type":"markdown","id":"355755af-3f61-4092-86be-dc71b237f0c8","metadata":{"id":"355755af-3f61-4092-86be-dc71b237f0c8"},"source":["b. For each dataset in the first dataframe, count the number of file extensions.\n","\n","i. Store these values as JSON in the column `file_extensions`. For example, if a dataset has 100 `.tif`s and 1000 `.jpeg`s, then you should store the string `{'tif':100,'jpeg':1000 }`.\n","\n","ii. If a dataset has a file extension that is `None` or `Nan` store it as `other`. For example, `{'tif':100,'jpeg':1000, 'other':4 }`."]},{"cell_type":"code","execution_count":null,"id":"f5950733-93ff-45bd-b093-8db95bc652cc","metadata":{"id":"f5950733-93ff-45bd-b093-8db95bc652cc"},"outputs":[],"source":["#INSERT CODE HERE\n","\n","def check_null(s):\n","    if str(s) == 'nan' or s==None:\n","        return True\n","    else:\n","        return False\n","\n","temp_file = 'datasets.tsv'\n","\n","if Path(temp_file).exists():                         \n","    df2 = pd.read_csv(temp_file, sep='\\t', low_memory=False)\n","else:\n","    # if temp_file does not exist, populate df2['file_extensions']\n","    exts = []\n","    for i in tqdm(df['dataset_uuid'].unique()):\n","        #print(i)\n","        df_temp = df[df['dataset_uuid']==i]\n","        df_temp['file_extension'] = df_temp['file_extension'].apply(lambda f: 'other' if check_null(f) else f)\n","        extensions = str(df_temp['file_extension'].value_counts().to_json())\n","        #print(extensions)\n","        exts.append(extensions)\n","    df2['file_extensions'] = exts"]},{"cell_type":"code","execution_count":null,"id":"389c43e4","metadata":{"id":"389c43e4"},"outputs":[],"source":["print(df2.shape)\n","df2.head()"]},{"cell_type":"markdown","id":"aa4b393a-f7cd-42ca-b6db-7e7c75123b66","metadata":{"id":"aa4b393a-f7cd-42ca-b6db-7e7c75123b66"},"source":["c. Save the dataframe to disk as a pickle file and as a tsv file. Save it to `datasets.tsv` and `datasets.pkl`."]},{"cell_type":"code","execution_count":null,"id":"5324aa67-9b36-4a50-8eb0-fe6edbd06378","metadata":{"id":"5324aa67-9b36-4a50-8eb0-fe6edbd06378"},"outputs":[],"source":["#INSERT CODE HERE\n","# save to .tsv file\n","df2.to_csv('datasets2.tsv', sep=\"\\t\",index=False)\n","\n","# save to .pkl file\n","import pickle\n","with open('datasets2.pkl', 'wb') as handle:\n","    pickle.dump(df2, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","id":"de4e2af4-0e5c-479f-afdb-8a7efbced120","metadata":{"id":"de4e2af4-0e5c-479f-afdb-8a7efbced120"},"source":["## Exercise 3\n","a. Create another dataframe with columns `['timestamp', 'number_of_datasets', 'number_of_collections', 'number_of_files', 'file_extensions']`.\n","\n","i. Timestamp records the current date, not the time, e.g. 2022-01-23.\n","\n","ii. The number of datasets in the `datasets` dataframe goes in `number_of_datasets`.\n","\n","iii. The number of unique collections in the `datasets` dataframe goes in `number_of_collections`.\n","\n","iv. The total number of files in the `manifest` dataframe goes in `number_of_files`.\n","\n","v. Another tricky one. Aggregate all the file extensions in column `file_extensions` in dataframe `datasets`, collect these as a single JSON block and it to `file_extensions` in this dataframe."]},{"cell_type":"code","execution_count":null,"id":"bad3aa71-e8cd-4a5b-a652-511ca156b854","metadata":{"id":"bad3aa71-e8cd-4a5b-a652-511ca156b854"},"outputs":[],"source":["#INSERT CODE HERE\n","temp_file = 'info.tsv'\n","\n","if Path(temp_file).exists():                         \n","  df3 = pd.read_csv(temp_file, sep='\\t', low_memory=False)\n","else:\n","  # Get timestamp\n","  tsp = str(datetime.datetime.now())[0:10]\n","  print(tsp)\n","\n","  # Get number of datasets\n","  nDatasets = df2['dataset_uuid'].nunique()\n","  print(nDatasets)\n","\n","  # Get number of collections\n","  nCollections = df2['file_extensions'].nunique()\n","  print(nCollections)\n","\n","  # Get number of files\n","  nFiles = df['filepath'].nunique()\n","  print(nFiles)\n","\n","  # Get all file_extensions\n","  df['file_extension'] = df['file_extension'].apply(lambda f: 'other' if check_null(f) else f)\n","  Exts = str(df['file_extension'].value_counts().to_json())\n","  print(Exts)\n","\n","  # assign values to lists.  \n","  data = [{'timestamp':tsp, 'number_of_datasets':nDatasets, 'number_of_collections':nCollections, 'number_of_files':nFiles, 'file_extensions':Exts}]  \n","  \n","  # create DataFrame.  \n","  df3 = pd.DataFrame(data)  \n","\n","  # convert type of 'timestamp' to type datetime\n","  df3['timestamp'] = pd.to_datetime(df3['timestamp'],errors='coerce')"]},{"cell_type":"code","source":["print(df3.shape)\n","df3.head()"],"metadata":{"id":"KDnUdA3TAywC"},"id":"KDnUdA3TAywC","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"01c3a707-53f7-4223-b81b-7f7eea7efc66","metadata":{"id":"01c3a707-53f7-4223-b81b-7f7eea7efc66"},"source":["b. Save the dataframe to disk as a pickle file and as a tsv file. Save it to `info.tsv` and `info.pkl`."]},{"cell_type":"code","execution_count":null,"id":"0f43b113-ee1b-4caf-9796-e93d4f49a26e","metadata":{"id":"0f43b113-ee1b-4caf-9796-e93d4f49a26e"},"outputs":[],"source":["#INSERT CODE HERE\n","# save to .tsv file\n","df3.to_csv('info.tsv', sep=\"\\t\",index=False)\n","\n","# save to .pkl file\n","import pickle\n","with open('info.pkl', 'wb') as handle:\n","    pickle.dump(df3, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","id":"f73b4520-6ee1-4515-8598-56bf6f025642","metadata":{"id":"f73b4520-6ee1-4515-8598-56bf6f025642"},"source":["## Exercise 4\n","Create some pretty plots and embed them in this notebook. Feel free to use any plotting library in Python. Make them pretty though. \n","\n","a. Waffleplot. Create a waffleplot from the column `file_extensions` in the dataframe `info`. \n","\n","* Title should be the date in `timestamp`.\n","* No axis labels.\n","* Add legend.\n","\n","**Hints***\n","* Use `pywaffle`."]},{"cell_type":"code","execution_count":null,"id":"40443223-850c-4d8f-8c39-1da4b16c0c4a","metadata":{"id":"40443223-850c-4d8f-8c39-1da4b16c0c4a"},"outputs":[],"source":["#INSERT CODE HERE\n","import matplotlib.pyplot as plt\n","from pywaffle import Waffle\n","import json\n","\n","temp_file = 'info.tsv'\n","\n","if Path(temp_file).exists():                         \n","    df3 = pd.read_csv(temp_file, sep='\\t', low_memory=False)\n","\n","# json need to convert to dataframe for plot     \n","string = df3['file_extensions'][0]\n","\n","# convert string to  object\n","d = json.loads(string)\n","\n","# convert json to dataframe\n","Extension = pd.DataFrame.from_dict(d, orient='index', columns=['count']).reset_index() # pd.read_json(json_obj, orient ='index')\n","Extension.rename(columns={\"index\": \"file_extension\"},inplace=True)\n","Extension['reduce'] = round(Extension['count']/100) # scale number of files to plot\n","print(Extension)\n","\n","\n","# Plot the waffle Chart\n","fig = plt.figure(\n","    figsize = (20, 20),\n","    FigureClass = Waffle,\n","    rows = 200,\n","    values = list(Extension['file_extension']),\n","    labels = list(Extension['index']),\n","    title = {\n","        'label': df3['timestamp'][0],\n","        'loc': 'center',\n","        'fontdict': {\n","            'fontsize': 20\n","        }\n","    },\n","    cmap_name='tab20b',\n","    legend={\n","        'loc': 'upper left',\n","        'bbox_to_anchor': (1, 1),\n","        'ncol': 3,\n","        'framealpha': 0,\n","        'fontsize': 10\n","    }\n",")\n","\n","fig.show()"]},{"cell_type":"markdown","id":"743d9451-4d90-476e-bc87-0ca1b1eb69c6","metadata":{"id":"743d9451-4d90-476e-bc87-0ca1b1eb69c6"},"source":["b. Histogram. Make a histogram using the column `download_ready` from the dataframe `manifest`. \n","\n","* Set title to `Broken links`\n","* Values set to `False` should be labeled as `Broken`.\n","* Values set to `True` should be labeled as `Not Broken`.\n","* No legend.\n","* Set y-label to `Number of links`"]},{"cell_type":"code","execution_count":null,"id":"2317cea2-3823-4164-a00d-f669dd575f16","metadata":{"id":"2317cea2-3823-4164-a00d-f669dd575f16"},"outputs":[],"source":["#INSERT CODE HERE\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","temp_file = 'manifest.tsv'\n","\n","if Path(temp_file).exists():                         \n","    df = pd.read_csv(temp_file, sep='\\t', low_memory=False)\n","\n","# Initialize figure size\n","plt.figure(figsize = ( 6 , 8))\n","  \n","# Plot histogram\n","fig = sns.histplot(data=df, stat='count', x='download_ready',bins=2)\n","fig.set(xlabel=None)\n","fig.set(xticklabels=[])\n","fig.tick_params(bottom=False) \n","fig.set(ylabel='Number of links')\n","fig.set(title='Broken links')\n","\n","# Display figure\n","plt.show()"]},{"cell_type":"markdown","id":"342b3d80-f63e-451f-b0b4-f363ac066ce5","metadata":{"id":"342b3d80-f63e-451f-b0b4-f363ac066ce5"},"source":["c. Pie chart. Create a pie chart using the values in `status_code` in the dataframe `manifest`. \n","\n","* Title should be `Status codes`.\n","* Add legends."]},{"cell_type":"code","execution_count":null,"id":"6337c178-0705-4ea1-a5c5-65a193da41a9","metadata":{"id":"6337c178-0705-4ea1-a5c5-65a193da41a9"},"outputs":[],"source":["#INSERT CODE HERE\n","# get dataset\n","file = 'manifest.tsv'\n","if Path(file).exists():\n","    df2 = pd.read_csv(file, sep='\\t', low_memory=False )\n","\n","    # Set colors\n","    cmako = sns.color_palette('bright')\n","\n","    # Set up data\n","    status = pd.DataFrame(df['status_code'].value_counts().reset_index())\n","    # status['isBroken'] = ['Unreachable', 'Reachable']\n","    print(status)\n","\n","    # Create pie chart\n","    plt.pie(data=status,x='status_code',labels='index', autopct='%.1f%%',colors = cmako)\n","    plt.title('Status code')\n","    plt.show()\n","else:\n","    print(file, ' does not exist!!!')"]},{"cell_type":"markdown","id":"9ebf717a-2376-49d3-b88c-e2ce53670b5d","metadata":{"id":"9ebf717a-2376-49d3-b88c-e2ce53670b5d"},"source":["d. Create a plot using the values in `file_creation_date` in dataframe manifest. However, this exercise is open-ended. \n","\n","This is my user-story and your job is to create the best plot you think will show what I want.\n","\n","* The only part I care about the `file_create_date` is the date. Or months, or years, not sure.\n","* What I want to do is to create a plot that I can show the increment of data through the years.\n","* I am still debating whether I should just show how many files are available per year, or do a cumulative plot since it should be, in theory, monotonically increasing.\n","\n","What do you think? Make a plot, convince me your plot is the best."]},{"cell_type":"code","execution_count":null,"id":"d982d560-4138-4d4f-b38d-f24e80f78cdf","metadata":{"id":"d982d560-4138-4d4f-b38d-f24e80f78cdf"},"outputs":[],"source":["#INSERT CODE HERE\n","# histogram plot for count of unique 'file_creation_date'\n","# y_axis \n","\n","df['file_creation_date_only'] = df['file_creation_date'][0:10]\n","# df['file_creation_date_only'] = pd.to_datetime(df3['file_creation_date_only'],errors='coerce')\n","\n","# Initialize figure size\n","plt.figure(figsize = ( 6 , 8))\n","  \n","# Plot histogram\n","fig = sns.histplot(data=df, stat='count', x='file_creation_date_only')\n","fig.set(xlabel='Time')\n","# fig.set(xticklabels=[])\n","fig.tick_params(bottom=False) \n","fig.set(ylabel='Number of files')\n","fig.set(title='Number of files across time')\n","\n","# Display figure\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}